{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#states = pd.read_csv('state abbr.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Corona_NLP_train.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Make text lowercase, remove text in square brackets, remove links, remove punctuation and remove words containing numbers\n",
    "    # Use re.sub to substitude all that with ''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)                              # remove text in square brackets\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)                # remove links\n",
    "    text = re.sub('<.*?>+', '', text)                               # remove text in angle brackets\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n",
    "    text = re.sub('\\n', '', text)                                   # remove line break\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)                             # remove words containing numbers\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = re.sub(emoji_pattern, '', text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['OriginalTweet'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA for all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data_words = list(sent_to_words(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.083*\"price\" + 0.045*\"food\" + 0.025*\"oil\" + 0.025*\"demand\" + 0.018*\"need\" '\n",
      "  '+ 0.017*\"due\" + 0.014*\"buy\" + 0.014*\"market\" + 0.014*\"stock\" + '\n",
      "  '0.013*\"increase\"'),\n",
      " (1,\n",
      "  '0.039*\"amp\" + 0.023*\"help\" + 0.023*\"time\" + 0.020*\"new\" + 0.019*\"crisis\" + '\n",
      "  '0.017*\"good\" + 0.017*\"use\" + 0.016*\"keep\" + 0.013*\"still\" + 0.013*\"covid\"'),\n",
      " (2,\n",
      "  '0.040*\"online\" + 0.031*\"worker\" + 0.029*\"shopping\" + 0.020*\"business\" + '\n",
      "  '0.018*\"shop\" + 0.017*\"high\" + 0.015*\"customer\" + 0.014*\"delivery\" + '\n",
      "  '0.012*\"support\" + 0.012*\"order\"'),\n",
      " (3,\n",
      "  '0.100*\"consumer\" + 0.048*\"pandemic\" + 0.023*\"impact\" + 0.017*\"company\" + '\n",
      "  '0.014*\"sanitizer\" + 0.013*\"behavior\" + 0.013*\"show\" + 0.013*\"economy\" + '\n",
      "  '0.012*\"hand\" + 0.010*\"sale\"'),\n",
      " (4,\n",
      "  '0.064*\"coronavirus\" + 0.038*\"store\" + 0.035*\"grocery\" + 0.033*\"supermarket\" '\n",
      "  '+ 0.028*\"go\" + 0.026*\"people\" + 0.019*\"get\" + 0.016*\"see\" + 0.016*\"make\" + '\n",
      "  '0.015*\"work\"')]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
    "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "])\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
    "#it will print the words with their frequencies.\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus, id2word=id2word, num_topics=5, random_state=100, \n",
    "   update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True\n",
    ")\n",
    "pprint(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic#0：\t\n",
      "Words：\t\n",
      "price\n",
      "food\n",
      "oil\n",
      "demand\n",
      "need\n",
      "\n",
      "Prob：\t [0.08315818 0.04487643 0.02505431 0.02488246 0.01811617]\n",
      "Topic#1：\t\n",
      "Words：\t\n",
      "amp\n",
      "help\n",
      "time\n",
      "new\n",
      "crisis\n",
      "\n",
      "Prob：\t [0.03858849 0.0234146  0.02314532 0.01958633 0.0185634 ]\n",
      "Topic#2：\t\n",
      "Words：\t\n",
      "online\n",
      "worker\n",
      "shopping\n",
      "business\n",
      "shop\n",
      "\n",
      "Prob：\t [0.03963828 0.03146422 0.02889453 0.02009588 0.01834049]\n",
      "Topic#3：\t\n",
      "Words：\t\n",
      "consumer\n",
      "pandemic\n",
      "impact\n",
      "company\n",
      "sanitizer\n",
      "\n",
      "Prob：\t [0.09952403 0.04770662 0.02297371 0.01746472 0.01354481]\n",
      "Topic#4：\t\n",
      "Words：\t\n",
      "coronavirus\n",
      "store\n",
      "grocery\n",
      "supermarket\n",
      "go\n",
      "\n",
      "Prob：\t [0.0642093  0.03803531 0.03483826 0.03271723 0.0278498 ]\n"
     ]
    }
   ],
   "source": [
    "    for topic_id in range(5):\n",
    "        print('Topic#%d：\\t' % topic_id)\n",
    "        term_distribute_all = lda_model.get_topic_terms(topicid=topic_id)\n",
    "        term_distribute = term_distribute_all[:5]\n",
    "        term_distribute = np.array(term_distribute)\n",
    "        term_id = term_distribute[:, 0].astype(np.int)\n",
    "        print('Words：\\t',)\n",
    "        for t in term_id:\n",
    "            print(id2word.id2token[t],)\n",
    "        print('\\nProb：\\t', term_distribute[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.849486445552428\n",
      "\n",
      "Coherence Score:  0.31383536527474076\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "# how good the model is. The lower the score the better the model will be\n",
    "\n",
    "coherence_model_lda = CoherenceModel(\n",
    "   model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "#the average /median of the pairwise word-similarity scores of the words in the topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA for each labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_positive=data['text'][data['Sentiment']=='Positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_negative=data['text'][data['Sentiment']=='Negative']\n",
    "\n",
    "data_EP=data['text'][data['Sentiment']=='Extremely Positive']\n",
    "\n",
    "data_EN=data['text'][data['Sentiment']=='Extremely Negative']\n",
    "data_neutral=data['text'][data['Sentiment']=='Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neutral=data_neutral.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(df):\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "    data_words = list(sent_to_words(df))\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc))\n",
    "                 if word not in stop_words] for doc in texts]\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent))\n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "            return texts_out\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
    "   'NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    corpus =[id2word.doc2bow(text) for text in texts] \n",
    "    [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=1, random_state=100, \n",
    "    update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "    pprint(lda_model.print_topics())\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.059*\"phone\" + 0.059*\"number\" + 0.059*\"neighbour\" + 0.039*\"supply\" + '\n",
      "  '0.039*\"poss\" + 0.039*\"online\" + 0.039*\"order\" + 0.039*\"shopping\" + '\n",
      "  '0.039*\"regular\" + 0.039*\"set\"')]\n",
      "\n",
      "Perplexity:  -3.510801986411766\n",
      "\n",
      "Coherence Score:  0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "topic_positive=get_topics(data_positive.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.091*\"online\" + 0.061*\"prefer\" + 0.061*\"time\" + 0.061*\"thing\" + '\n",
      "  '0.061*\"stop\" + 0.061*\"spread\" + 0.061*\"shopping\" + 0.061*\"also\" + '\n",
      "  '0.061*\"payment\" + 0.061*\"home\"')]\n",
      "\n",
      "Perplexity:  -3.115619389449849\n",
      "\n",
      "Coherence Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "topic_negative=get_topics(data_negative.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.086*\"time\" + 0.057*\"particularly\" + 0.057*\"pack\" + 0.057*\"thank\" + '\n",
      "  '0.057*\"situation\" + 0.057*\"share\" + 0.057*\"patience\" + 0.057*\"wait\" + '\n",
      "  '0.057*\"due\" + 0.057*\"may\"')]\n",
      "\n",
      "Perplexity:  -3.1786561409632363\n",
      "\n",
      "Coherence Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "topic_EP=get_topics(data_EP.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.053*\"confinement\" + 0.053*\"stayathome\" + 0.053*\"serious\" + '\n",
      "  '0.053*\"restezchezvous\" + 0.053*\"ready\" + 0.053*\"panic\" + '\n",
      "  '0.053*\"supermarket\" + 0.053*\"stock\" + 0.053*\"shortage\" + 0.053*\"be\"')]\n",
      "\n",
      "Perplexity:  -3.3123766319616776\n",
      "\n",
      "Coherence Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "topic_EN=get_topics(data_EN.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.250*\"buy\" + 0.250*\"rebel\" + 0.250*\"supermarket\" + 0.250*\"today\"')]\n",
      "\n",
      "Perplexity:  -1.683350554824898\n",
      "\n",
      "Coherence Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "#data_neutral=data['text'][data['Sentiment']=='Neutral']\n",
    "topic_neutral=get_topics(data_neutral.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI Model for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = pd.read_pickle(r'week_words.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weekly = object.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-56-0af2f55d90fe>:1: DeprecationWarning: invalid escape sequence \\s\n",
      "  data_weekly = list(filter(lambda ele: re.search(\"[a-zA-Z\\s]+\", ele) is not None, data_weekly))\n"
     ]
    }
   ],
   "source": [
    "data_weekly = list(filter(lambda ele: re.search(\"[a-zA-Z\\s]+\", ele) is not None, data_weekly)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_po=\"\".join(data_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ne=\"\".join(data_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ep=\"\".join(data_EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en=\"\".join(data_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models,similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simi(s1,s2):\n",
    "    dictionary= corpora.Dictionary([s1.split()])\n",
    "    texts = s1\n",
    "\n",
    "    corpus =[dictionary.doc2bow(text) for text in [s1.split()]] \n",
    "\n",
    "    lsi=models.LdaModel(corpus, id2word=dictionary, num_topics=4)\n",
    "\n",
    "    vec_bow=dictionary.doc2bow(s2.lower().split())\n",
    "    vec_lsi=lsi[vec_bow]  #convert the query to LSI space\n",
    "\n",
    "     \n",
    "    #transform corpus to space and index it\n",
    "    index=similarities.MatrixSimilarity(lsi[corpus])\n",
    "     \n",
    "\n",
    "    sims=index[vec_lsi]#?\n",
    "    sims=sorted(enumerate(sims),key=lambda item:-item[1])\n",
    "     \n",
    "  \n",
    "    print(np.mean(sims))\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48889344930648804\n"
     ]
    }
   ],
   "source": [
    "simi(data_po,data_weekly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49993452429771423\n"
     ]
    }
   ],
   "source": [
    "simi(data_ne,data_weekly[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49908721446990967\n"
     ]
    }
   ],
   "source": [
    "simi(data_en,data_weekly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47187837958335876\n"
     ]
    }
   ],
   "source": [
    "simi(data_ep,data_weekly[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49930670857429504\n",
      "0.4967852830886841\n",
      "0.4999352991580963\n",
      "0.47077518701553345\n",
      "0.499090313911438\n",
      "0.48993760347366333\n",
      "0.4995453953742981\n",
      "0.4983910620212555\n",
      "0.48360931873321533\n",
      "0.4998330771923065\n",
      "0.49934902787208557\n",
      "0.4994356036186218\n",
      "0.4998909533023834\n",
      "0.4998686909675598\n",
      "0.4742996096611023\n",
      "0.4893410801887512\n",
      "0.49919113516807556\n",
      "0.4777284860610962\n",
      "0.4988222122192383\n",
      "0.4811556041240692\n",
      "0.4994298219680786\n",
      "0.49996238946914673\n",
      "0.49725213646888733\n",
      "0.49999603629112244\n",
      "0.5\n",
      "0.49837130308151245\n",
      "0.49297085404396057\n",
      "0.49610435962677\n",
      "0.4975164830684662\n",
      "0.49300384521484375\n",
      "0.48386019468307495\n",
      "0.48928865790367126\n",
      "1 3\n",
      "0.48490503430366516\n",
      "0.4975071847438812\n",
      "0.48314717411994934\n",
      "0.4964180588722229\n",
      "0.4969451427459717\n",
      "0.49959152936935425\n",
      "0.49926841259002686\n",
      "0.49974578619003296\n",
      "0.49993696808815\n",
      "0.4642929136753082\n",
      "0.4953835606575012\n",
      "0.4999728798866272\n",
      "0.49311941862106323\n",
      "0.49942702054977417\n",
      "0.49621981382369995\n",
      "0.4970610439777374\n",
      "0.4955645203590393\n",
      "0.49993985891342163\n",
      "0.4999181926250458\n",
      "0.49998658895492554\n",
      "1 23\n",
      "0.49984312057495117\n",
      "0.49999186396598816\n",
      "0.48761728405952454\n",
      "0.499966025352478\n",
      "0.4999200701713562\n",
      "0.4983353316783905\n",
      "0.5\n",
      "0.499329149723053\n",
      "0.46781235933303833\n",
      "0.4996078610420227\n",
      "0.4998732805252075\n",
      "2 5\n",
      "0.5\n",
      "0.49985355138778687\n",
      "0.4945976138114929\n",
      "0.49996137619018555\n",
      "0.49539369344711304\n",
      "0.49698328971862793\n",
      "0.4971138834953308\n",
      "0.49932345747947693\n",
      "0.49631407856941223\n",
      "0.4952903985977173\n",
      "0.4943162798881531\n",
      "0.48476171493530273\n",
      "0.4917232394218445\n",
      "0.49316078424453735\n",
      "0.4977220296859741\n",
      "0.4945666790008545\n",
      "0.44691526889801025\n",
      "0.49969786405563354\n",
      "0.49961236119270325\n",
      "0.4998726546764374\n",
      "0.49544715881347656\n",
      "0.49889272451400757\n",
      "0.4998798370361328\n",
      "0.47223490476608276\n",
      "0.4954017400741577\n",
      "0.49997326731681824\n",
      "0.47941964864730835\n",
      "0.4999559819698334\n",
      "0.49839162826538086\n",
      "0.4917336702346802\n",
      "0.4999922811985016\n",
      "0.49724042415618896\n",
      "0.4998718202114105\n",
      "0.4893847703933716\n",
      "0.49767160415649414\n",
      "0.4806254804134369\n",
      "3 12\n",
      "0.49485552310943604\n",
      "0.41655904054641724\n",
      "0.4998250901699066\n",
      "0.49258944392204285\n",
      "0.4878007173538208\n",
      "0.49963387846946716\n",
      "0.4986754059791565\n",
      "0.49474287033081055\n",
      "0.4925850033760071\n",
      "0.4948117733001709\n",
      "0.4999997913837433\n",
      "0.5\n",
      "0.4905463755130768\n",
      "0.4774644672870636\n",
      "0.4993446171283722\n",
      "0.49987509846687317\n",
      "0.4998418986797333\n"
     ]
    }
   ],
   "source": [
    "list = [data_po,data_ne,data_ep, data_en]\n",
    "\n",
    "for i in range(len(list)):\n",
    "        for j in range(len(data_weekly)):\n",
    "            try:\n",
    "                dt=simi(list[i], data_weekly[j])\n",
    "            except:\n",
    "                try:\n",
    "                    print(i,j)\n",
    "                    dt=simi(list[i], data_weekly[j])\n",
    "                except:\n",
    "                    print(i,j)\n",
    "                    dt=simi(list[i], data_weekly[j])\n",
    "            #dt=simi(list[i], data_weekly[j])\n",
    "            dt=pd.DataFrame(dt)\n",
    "            dt.to_csv('dt.csv',index=False)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
